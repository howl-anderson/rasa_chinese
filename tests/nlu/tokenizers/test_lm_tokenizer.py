import pytest
from typing import Dict, Optional

import rasa.shared.utils.io
from rasa.nlu.constants import TOKENS_NAMES
from rasa.shared.nlu.constants import TEXT, INTENT, ACTION_TEXT, ACTION_NAME
from rasa.shared.nlu.training_data.training_data import TrainingData
from rasa.shared.nlu.training_data.message import Message
from rasa_chinese.nlu.tokenizers.lm_tokenizer import LanguageModelTokenizer


def create_lm_tokenizer(config: Optional[Dict] = None) -> LanguageModelTokenizer:
    config = config if config else {}
    return LanguageModelTokenizer({**LanguageModelTokenizer.get_default_config(), **config})


@pytest.mark.parametrize(
    "text, expected_tokens, expected_indices",
    [
        (
            "æˆ‘æƒ³å»åƒå…°å·æ‹‰é¢",  # easy/normal case
            ["æˆ‘", "æƒ³", "å»", "åƒ", "å…°", "å·", "æ‹‰", "é¢"],
            [(0, 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, 6), (6, 7), (7, 8)],
        ),
        (
            "ä»ä¸œç•ˆæ‘èµ°äº†ã€‚",  # OOV case: `ç•ˆ` is a OOV word; TODO: not OOV anymore, fix it
            ["ä»", "ä¸œ", "ç•ˆ", "æ‘", "èµ°", "äº†", "ã€‚"],
            [(0, 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, 6), (6, 7)],
        ),
        (
            "Micheal ä½ å¥½å—ï¼Ÿ",  # Chinese mixed up with English
            ["Micheal", "ä½ ", "å¥½", "å—", "ï¼Ÿ"],
            [(0, 7), (8, 9,), (9, 10), (10, 11), (11, 12)],
        ),
        (
            "æˆ‘æƒ³ä¹° iPhone 12 ğŸ¤­",  # Chinese mixed up with English, numbers, and emoji
            ["æˆ‘", "æƒ³", "ä¹°", "iPhone", "12", "ğŸ¤­"],
            [(0, 1), (1, 2), (2, 3), (4, 10), (11, 13), (14, 15)],
        ),
    ],
)
def test_lm(text, expected_tokens, expected_indices):

    tk = create_lm_tokenizer()

    tokens = tk.tokenize(Message.build(text=text), attribute=TEXT)

    assert [t.text for t in tokens] == expected_tokens
    assert [t.start for t in tokens] == [i[0] for i in expected_indices]
    assert [t.end for t in tokens] == [i[1] for i in expected_indices]

